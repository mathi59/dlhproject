{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python3 Generating Test Results For Reproducibiity Study:\n",
    "\n",
    "## Journal PLOS ONE Extracting Schemas from Thought Records [paper](https://doi.org/10.1371/journal.pone.0257832)\n",
    "\n",
    "<br>\n",
    "The purpose of this script is to generate the results using the models supporting Hypothesis 1 of the study. \n",
    "The wall time of runtimes are provided in the first comment of cells of time intensive code. Additionally, the cell magic \"%%time\" in these cells ensures that runtimes are printed so that these can be compared to the reported runtimes to get appropriate estimates when running on a different machine.<br>\n",
    "\n",
    "<br>\n",
    "The modules below need to be installed before running the code:\n",
    "    <ol>\n",
    "    <li>numpy==1.21.6</li>\n",
    "    <li>pandas==1.3.5</li>\n",
    "    <li>gensim==3.6.0</li>\n",
    "    <li>statsmodels==0.10.2</li>\n",
    "    <li>rasa-nlu</li>\n",
    "    <li>tensorflow==2.8.0</li>\n",
    "    <li>talos</li>\n",
    "    <li>scipy==1.4.1</li>\n",
    "    <li>scikit-learn==0.23.2</li>\n",
    "    </ol>\n",
    "\n",
    "<br>\n",
    "The following inputs are required and can be found in the group165/Data2 directory:\n",
    "    <ol>\n",
    "    <li>glove.6B directory</li>\n",
    "    <li>DatasetsForH1 directory</li>\n",
    "    <li>mlm_rnn.zip file</li>\n",
    "    <li>MLMs directory</li>\n",
    "    <li>PSMs directory</li>\n",
    "    </ol>\n",
    "\n",
    "We import the following packages and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-05 10:32:41.253436: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /var/folders/m0/lhqg5pdd2s33k5q0kwnt4518h2f2xw/T/ipykernel_15299/347793014.py:46: The name tf.keras.backend.set_session is deprecated. Please use tf.compat.v1.keras.backend.set_session instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#set seed\n",
    "seed = 57839\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "import sys\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import functools\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn import metrics, preprocessing, svm\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "from tensorflow.python.keras.metrics import Metric\n",
    "from tensorflow import keras\n",
    "import talos\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Flatten, Embedding, SimpleRNN, LSTM, GRU, Bidirectional,Dropout\n",
    "\n",
    "from keras import backend as K\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.13 (default, Mar 28 2022, 07:24:34) \n",
      "[Clang 12.0.0 ]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim==3.6.0\n",
      "keras==2.8.0\n",
      "tensorflow==2.8.0\n",
      "numpy==1.21.6\n",
      "scipy==1.4.1\n",
      "pandas==1.3.5\n",
      "talos==1.2.3\n",
      "scikit-learn==0.23.2\n",
      "statsmodels==0.13.2\n"
     ]
    }
   ],
   "source": [
    "#list packages and their version numbers as used in this script (code is taken from \n",
    "#https://stackoverflow.com/questions/40428931/package-for-listing-version-of-packages-used-in-a-jupyter-notebook)\n",
    "import pkg_resources\n",
    "import types\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "\n",
    "        # Some packages are weird and have different\n",
    "        # imported names vs. system/pip names. Unfortunately,\n",
    "        # there is no systematic way to get pip names from\n",
    "        # a package's imported name. You'll have to add\n",
    "        # exceptions to this list manually!\n",
    "        poorly_named_packages = {\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "# The only way I found to get the version of the root package\n",
    "# from only the name of the package is to cross-check the names \n",
    "# of installed packages vs. imported packages\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We also set the working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/sampathg/Documents/MyUIUC/CS598-DLHC/group165\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the datasets from csv\n",
    "> The preprocessed utterances are split into three sets in the R script. They are saved in three separate csv files. Additionally, the manually assigned labels that correspond with the utterances are saved in three separate csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in datasets (already pre-processed)\n",
    "def readcsv(fname,istext):\n",
    "    if istext:\n",
    "        with open(fname,'rt') as f:\n",
    "            reader=csv.reader(f)\n",
    "            next(reader)\n",
    "            data = []\n",
    "            for row in reader:\n",
    "                for item in row:\n",
    "                    data.append(item)\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(fname,'r') as f:\n",
    "            reader=csv.reader(f,delimiter=';')\n",
    "            next(reader)\n",
    "            data = list(reader)\n",
    "            data = np.asarray(data, dtype='int')\n",
    "            f.close()\n",
    "    return data \n",
    "\n",
    "# read in training, validation, and test set utterances\n",
    "train_text = readcsv('Data2/DatasetsForH1/H1_train_texts.csv',True)\n",
    "val_text = readcsv('Data2/DatasetsForH1/H1_validate_texts.csv', True)\n",
    "test_text = readcsv('Data2/DatasetsForH1/H1_test_texts.csv',True)\n",
    "\n",
    "# read in training, validation, and test set labels\n",
    "train_labels = readcsv('Data2/DatasetsForH1/H1_train_labels.csv',False)[:,0:9]\n",
    "val_labels = readcsv('Data2/DatasetsForH1/H1_validate_labels.csv', False)[:,0:9]\n",
    "test_labels = readcsv('Data2/DatasetsForH1/H1_test_labels.csv',False)[:,0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lot people may think well lot people might not like me', 'might not working fast enough their standards', 'may not able graduate', 'would get bad performance review', 'friends will get annoyed by me']\n"
     ]
    }
   ],
   "source": [
    "print(train_text[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 0 0 0 0 0 3]\n",
      " [0 3 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 3 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0]\n",
      " [0 0 3 0 0 0 0 0 0]\n",
      " [2 0 2 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(test_labels[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As can be seen, some utterances have multiple schemas assigned. However, overall, the label matrices are sparse matrices. The first column of the labels corresponds to the \"Attachment\" schema, the second to the \"Competence\" schema, the third to last to the \"Other's views on self\" schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for later use\n",
    "schemas = [\"Attach\",\"Comp\",\"Global\",\"Health\",\"Control\",\"MetaCog\",\"Others\",\"Hopeless\",\"OthViews\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding the utterances using GLoVE\n",
    "> We have opted for representing the words in utterances as word vectors. We adopt the GLoVE word vector space that has been created with Wikipedia 2014. First, we tokenize the top 2000 words of the training set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624\n"
     ]
    }
   ],
   "source": [
    "# prepare tokenizer\n",
    "max_words = 2000\n",
    "t = Tokenizer(num_words = max_words)\n",
    "t.fit_on_texts(train_text)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The tokenizer takes the words and indexes these based on frequency. For the recurrent neural net, we need padded utterances sequences. Texts_to_sequences simply represents each utterance as a vector of tokens. Padding ensures that all vectors are of the same length, by appending 0s to the end of shorter vectors. We pad to a length of 25 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[147, 28, 48, 37, 101, 147, 28, 32, 1, 8, 5], [32, 1, 155, 658, 14, 125, 568], [48, 1, 19, 448], [2, 11, 53, 449, 659], [50, 6, 11, 373, 98, 5]]\n"
     ]
    }
   ],
   "source": [
    "# integer encode all utterances\n",
    "encoded_train = t.texts_to_sequences(train_text)\n",
    "encoded_validate = t.texts_to_sequences(val_text)\n",
    "encoded_test = t.texts_to_sequences(test_text)\n",
    "\n",
    "# pad documents to a max length of 25 words\n",
    "max_length = 25\n",
    "\n",
    "padded_train = pad_sequences(encoded_train, maxlen=max_length, padding='post')\n",
    "padded_validate = pad_sequences(encoded_validate, maxlen=max_length, padding='post')\n",
    "padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
    "\n",
    "print(encoded_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[147  28  48  37 101 147  28  32   1   8   5   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 32   1 155 658  14 125 568   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 48   1  19 448   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  2  11  53 449 659   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 50   6  11 373  98   5   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can now load the GLoVE embeddings into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "CPU times: user 8.15 s, sys: 322 ms, total: 8.47 s\n",
      "Wall time: 8.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 10sec\n",
    "# load all embeddings into memory\n",
    "embeddings_index = dict()\n",
    "f = open('Data2/glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can then create an embedding matrix by taking each word of the training set and finding the corresponding word vector in the GLoVE data. We only work with 100 dimensional representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dims = 100\n",
    "embedding_matrix = np.zeros((vocab_size, vec_dims))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.29214445 0.         ... 0.         0.         0.        ]\n",
      " [0.         1.29214445 0.         ... 0.         0.         0.        ]\n",
      " [0.         1.29214445 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         1.69021763 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "(4151, 2000)\n"
     ]
    }
   ],
   "source": [
    "# create tfidf weighted encoding matrix of utterances\n",
    "train_sequences = t.texts_to_matrix(train_text,mode='tfidf')\n",
    "val_sequences =  t.texts_to_matrix(val_text,mode='tfidf')\n",
    "test_sequences = t.texts_to_matrix(test_text,mode='tfidf')\n",
    "print(train_sequences[0:5])\n",
    "print(train_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to normalize the word vectors\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "       return v\n",
    "    return v / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create utterance embeddings as tfidf weighted average of normalized word vectors\n",
    "def seq2vec(datarow,embedmat):\n",
    "  #initialize an empty utterance vector of the same length as word2vec vectors\n",
    "  seqvec = np.zeros((100,))\n",
    "  #counter for number of words in a specific utterance\n",
    "  wordcount = 1\n",
    "  #we iterate over the 2000 possible words in a given utterance\n",
    "  wordind = 1\n",
    "  while (wordind < len(datarow)):\n",
    "    #the tf-idf weight is saved in the cells of datarow\n",
    "    tfidfweight = datarow[wordind]\n",
    "    if not tfidfweight is None:\n",
    "      wordembed = tfidfweight * embedmat[wordind,]\n",
    "      seqvec = seqvec + normalize(wordembed)\n",
    "      wordcount = wordcount + 1\n",
    "    wordind = wordind + 1\n",
    "  return seqvec/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through the matrix and embed each utterances\n",
    "def embed_utts(sequences,embedmat):\n",
    "  vecseq = [seq2vec(seq,embedmat)for seq in sequences]\n",
    "  return vecseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> we now have everything needed to create the utterance embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.44478099e-05  3.48539840e-04  3.35962753e-04 -3.70855457e-04\n",
      " -2.63147438e-04  1.07227074e-04 -1.66559109e-04  1.94234500e-05\n",
      "  7.42420570e-05 -1.80615841e-04  1.80387578e-05  4.92242034e-05\n",
      "  2.75006568e-04  2.34416192e-05  8.31148165e-05 -2.93833280e-04\n",
      " -7.15121389e-05  2.98592314e-04 -4.55134987e-04  4.72657153e-04\n",
      "  2.57585086e-04  1.69741478e-04  7.75960265e-05 -2.15817394e-04\n",
      " -4.34789085e-05  7.24571212e-05 -1.54585404e-04 -4.98166781e-04\n",
      "  1.93941088e-04 -1.74921206e-04  2.37557331e-05  4.85150809e-04\n",
      "  3.08554881e-05 -4.62293641e-05  1.35110613e-04  2.80284189e-04\n",
      " -3.22980711e-05  3.12968134e-04  8.27704500e-05 -2.40951546e-04\n",
      " -3.13527886e-04 -1.35440392e-04 -2.05195768e-05 -4.81099111e-04\n",
      " -2.75375333e-04 -1.27601856e-04  2.50256011e-04 -2.50631136e-04\n",
      " -1.51297680e-04 -8.33555219e-04  3.54382525e-05 -8.74190709e-05\n",
      "  1.05239327e-05  8.00132559e-04 -1.52039351e-04 -1.90058573e-03\n",
      "  9.49153013e-05 -1.17238522e-05  1.18845110e-03  3.93093667e-04\n",
      " -1.88908628e-04  9.94003983e-04 -3.57759952e-04  2.93419204e-05\n",
      "  6.71172261e-04  1.52662986e-04  5.76767087e-04  2.42848249e-04\n",
      "  9.63268891e-06 -2.31785203e-04  1.51988867e-05 -2.98388563e-04\n",
      " -3.74962639e-05 -3.26247156e-04  6.07764018e-05  6.39454626e-05\n",
      "  8.71421849e-05  6.76090198e-05 -5.16919711e-04 -2.17311368e-05\n",
      "  4.80646118e-04 -1.64901023e-04 -5.06686209e-04  1.66221153e-05\n",
      " -1.31359098e-03 -2.02032865e-04 -2.72980358e-05 -6.66373277e-05\n",
      " -3.30798167e-04 -3.81777162e-04 -2.22552903e-05 -2.22256701e-04\n",
      "  4.71648347e-05 -1.49116944e-04 -4.30563119e-04  4.04963585e-05\n",
      " -2.00736932e-04 -3.13451294e-04  2.94196617e-04  2.83769604e-04]\n",
      "CPU times: user 2min 4s, sys: 1.2 s, total: 2min 6s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 1min 14s\n",
    "# embedd all three datasets\n",
    "train_embedutts = embed_utts(train_sequences,embedding_matrix)\n",
    "val_embedutts = embed_utts(val_sequences,embedding_matrix)\n",
    "test_embedutts = embed_utts(test_sequences,embedding_matrix)\n",
    "print(train_embedutts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "> We use the Spearman correlation to evaluate the models and choose the best one, because it can be used for both the regression and the classification outcomes. This is not the case for a weighted Cohen's Kappa, for example, which only works for class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Goodness of Fit\n",
    "def gof_spear(X,Y):\n",
    "    #spearman correlation of columns (schemas)\n",
    "    gof_spear = np.zeros(X.shape[1])    \n",
    "    for schema in range(9):\n",
    "        rho,p = scipy.stats.spearmanr(X[:,schema],Y[:,schema])\n",
    "        gof_spear[schema]=rho\n",
    "    return gof_spear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest Neighbors Classification and Regression\n",
    "> Since we have ordinal labels for our data, we train both classification and regression algorithms and see which one performs better. We also have multi-label data, and therefore write a custom kNN algorithm. We use the cosine distance to find the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine distance\n",
    "def cosine_dist(X,Y):\n",
    "    return scipy.spatial.distance.cosine(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kNN algorithm\n",
    "def knn_custom(train_X,test_X,train_y,test_y,k,dist,classification):\n",
    "    #empty array to collect the results (should have shape of samples to classify)\n",
    "    votes = np.zeros(test_y.shape)\n",
    "    #fit the knn\n",
    "    knn=NearestNeighbors(k, metric=dist)\n",
    "    knn.fit(train_X)\n",
    "    #collect neighbors\n",
    "    i=0 # index to collect votes of the neighbors\n",
    "    for sample in test_X:\n",
    "        neighbors=knn.kneighbors([sample],k,return_distance=False)[0]\n",
    "        if classification:\n",
    "            output_y = np.zeros((k,test_y.shape[1]))\n",
    "            j=0\n",
    "            for neighbor in neighbors:\n",
    "                output_y[j,:] = train_y[neighbor,:]\n",
    "                j=j+1\n",
    "            votes[i,:] = stats.mode(output_y,nan_policy='omit')[0]\n",
    "        else:\n",
    "            output_y = np.zeros(test_y.shape[1])\n",
    "            for neighbor in neighbors:\n",
    "                output_y += train_y[neighbor,:]\n",
    "                votes[i,:] = np.divide(output_y,k)\n",
    "        i=i+1\n",
    "    return votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kNN(test_X,test_y,classification):\n",
    "    if classification:\n",
    "        my_knn=knn_custom(train_embedutts,test_X,train_labels,test_y,4,cosine_dist,1)\n",
    "    else:\n",
    "        my_knn=knn_custom(train_embedutts,test_X,train_labels,test_y,5,cosine_dist,0)\n",
    "    return gof_spear(my_knn,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 33s, sys: 3.45 s, total: 5min 36s\n",
      "Wall time: 5min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#wall time to run: ~ 4min\n",
    "output_kNN_class = my_kNN(test_embedutts,test_labels,1)\n",
    "output_kNN_reg = my_kNN(test_embedutts,test_labels,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classification Prediction\n",
      "          estimate\n",
      "Attach    0.550705\n",
      "Comp      0.690230\n",
      "Global    0.401123\n",
      "Health    0.742217\n",
      "Control   0.107526\n",
      "MetaCog        NaN\n",
      "Others    0.279105\n",
      "Hopeless  0.484137\n",
      "OthViews  0.454565\n"
     ]
    }
   ],
   "source": [
    "print('KNN Classification Prediction')\n",
    "print(pd.DataFrame(data=output_kNN_class,index=schemas,columns=['estimate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Regression Prediction\n",
      "          estimate\n",
      "Attach    0.626743\n",
      "Comp      0.663091\n",
      "Global    0.411444\n",
      "Health    0.534902\n",
      "Control   0.231541\n",
      "MetaCog   0.104785\n",
      "Others    0.243713\n",
      "Hopeless  0.513825\n",
      "OthViews  0.458473\n"
     ]
    }
   ],
   "source": [
    "print('KNN Regression Prediction')\n",
    "print(pd.DataFrame(data=output_kNN_reg,index=schemas,columns=['estimate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machine\n",
    "> The second algorithm we chose are support vector machines (SVMs). Again, we train both a support vector classification (SVC) and a support vectore regression (SVR). We only try all three types of standard kernels and do not do any additional parameter tuning. Just like the kNN, the support vector machine takes as input the utterances encoded as averages of word vectors. Support vector classification and regression do not allow for multilabel output. We therefore train disjoint models, one for each schema.<br>\n",
    "For both types of SVM, we first transform the input texts as the algorithm expects normally distributed input centered around 0 and with a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM/SVR\n",
    "def svm_scaler(train_X):\n",
    "        #scale the data\n",
    "        scaler_texts = StandardScaler()\n",
    "        scaler_texts = scaler_texts.fit(train_X)\n",
    "        return scaler_texts\n",
    "\n",
    "scaler_texts = svm_scaler(train_embedutts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Since SVMs, unlike kNNs, can be trained and reused, we write a method that returns all 9 models and a separate one for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_custom(train_X,train_y,text_scaler,kern,classification):\n",
    "        models=[]\n",
    "        train_X = text_scaler.transform(train_X)\n",
    "        #fit a new support vector regression for each schema\n",
    "        for schema in range(9):\n",
    "            if classification:\n",
    "                model = svm.SVC(kernel=kern)\n",
    "            else:\n",
    "                model = svm.SVR(kernel=kern)\n",
    "            model.fit(train_X, train_y[:,schema])\n",
    "            models.append(model)\n",
    "        return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighting model output (spearman correlations) by schema frequencies in training set and returning mean over schemas\n",
    "def performance(train_y,output):\n",
    "    train_y = np.array(train_y)\n",
    "    train_y[train_y>0]=1\n",
    "    weighting = train_y.sum(axis=0)/train_y.shape[0]\n",
    "    perf = output * weighting\n",
    "    return np.nanmean(np.array(perf), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_predict(svm_models,test_X,train_y,test_y,text_scaler):\n",
    "    #empty array to collect the results (should have shape of samples to classify)\n",
    "    votes = np.zeros(test_y.shape)\n",
    "    for schema in range(9):\n",
    "        svm_model=svm_models[schema]\n",
    "        prediction = svm_model.predict(text_scaler.transform(test_X))\n",
    "        votes[:,schema] = prediction\n",
    "    out = votes\n",
    "    gof = gof_spear(out,test_y)\n",
    "    perf = performance(train_y,gof)\n",
    "    return out,perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.6 s, sys: 147 ms, total: 13.8 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 45sec\n",
    "# svm\n",
    "svm_rbf_models =  svm_custom(train_embedutts,train_labels,scaler_texts,'rbf',1)\n",
    "svm_rbf_out, svm_rbf_perf = svm_predict(svm_rbf_models,val_embedutts,train_labels,val_labels,scaler_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13.1 s, sys: 143 ms, total: 13.3 s\n",
      "Wall time: 13.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 2min 20sec\n",
    "# svr\n",
    "svr_rbf_models =  svm_custom(train_embedutts,train_labels,scaler_texts,'rbf',0)\n",
    "svr_rbf_out, svr_rbf_perf = svm_predict(svr_rbf_models,val_embedutts,train_labels,val_labels,scaler_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.6 s, sys: 33.8 ms, total: 3.63 s\n",
      "Wall time: 3.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wall time to run: 4sec\n",
    "def my_svm(test_X,test_y,classification):\n",
    "    if classification:\n",
    "        my_svm_out, my_svm_perf=svm_predict(svm_rbf_models,test_X,train_labels,test_y,scaler_texts)\n",
    "    else:\n",
    "        my_svm_out, my_svm_perf=svm_predict(svr_rbf_models,test_X,train_labels,test_y,scaler_texts)\n",
    "    return gof_spear(my_svm_out,test_y)\n",
    "\n",
    "output_SVC = my_svm(test_embedutts,test_labels,1)\n",
    "output_SVR = my_svm(test_embedutts,test_labels,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Classification Prediction\n",
      "          estimate\n",
      "Attach    0.647714\n",
      "Comp      0.684661\n",
      "Global    0.357601\n",
      "Health    0.729181\n",
      "Control        NaN\n",
      "MetaCog        NaN\n",
      "Others         NaN\n",
      "Hopeless  0.489903\n",
      "OthViews  0.476297\n"
     ]
    }
   ],
   "source": [
    "print('SVM Classification Prediction')\n",
    "print(pd.DataFrame(data=output_SVC,index=schemas,columns=['estimate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Regression Prediction\n",
      "          estimate\n",
      "Attach    0.675340\n",
      "Comp      0.640866\n",
      "Global    0.489372\n",
      "Health    0.349064\n",
      "Control   0.310007\n",
      "MetaCog   0.114894\n",
      "Others    0.185827\n",
      "Hopeless  0.535979\n",
      "OthViews  0.516635\n"
     ]
    }
   ],
   "source": [
    "print('SVM Regression Prediction')\n",
    "print(pd.DataFrame(data=output_SVR,index=schemas,columns=['estimate']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We train two types of recurrent neural networks: a multilabel RNN that predicts all 9 schemas simultaneously and a set of 9 single-label RNNs that predict the labels for each schema separately. Each RNN consists of 4 layers: an embedding layer, a bidirectional LSTM layer, a dropout layer, and an output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we restore the deployed Talos experiment\n",
    "restore = talos.Restore('Data2/mlm_rnn.zip')  # Changed from /Data/mlm_rnn.zip\n",
    "#to get the best performing parameters, we get the results of the Talos experiment\n",
    "scan_results = restore.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   round_epochs     loss  mean_absolute_error  val_loss  \\\n",
      "7           100  1.26202             0.212742       0.0   \n",
      "\n",
      "   val_mean_absolute_error  batch_size  dropout  epochs  \\\n",
      "7                      0.0          32      0.1     100   \n",
      "\n",
      "                     losses  lstm_units optimizer  \n",
      "7  categorical_crossentropy         100      Adam  \n"
     ]
    }
   ],
   "source": [
    "#select the row with the smallest mean absolute error\n",
    "print(scan_results[scan_results.mean_absolute_error == scan_results.mean_absolute_error.min()]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We have learned that despite setting the random seed values for numpy and tensorflow, some variability remains with each training of the RNNs and our results will therefore not be 100% reproducible. To ensure that we cannot be accused of reporting just a \"lucky shot\", we have decided to follow the advice given in this blog post https://machinelearningmastery.com/reproducible-results-neural-networks-keras/ . We therefore train 30 multi-label neural nets with the best parameters identfied with the Talos scan. We report the mean Spearman correlations in the article. We do the same with the per-schema RNNs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    " #generate predictions with the per-schema models\n",
    "def predict_schema_mlm(test_text, test_labels,fixed=None):\n",
    "    if fixed is None:\n",
    "        all_preds = np.zeros((test_labels.shape[0],test_labels.shape[1],30)) \n",
    "        all_gofs = np.zeros((30,9)) \n",
    "        for j in range(30): \n",
    "            model_name = \"Data2/MLMs/mlm_\" + str(j)\n",
    "            model = keras.models.load_model(model_name + '.h5')\n",
    "            preds = model.predict(test_text)\n",
    "            gofs = gof_spear(preds,test_labels)\n",
    "            all_preds[:,:,j] = preds\n",
    "            all_gofs[j,:] = gofs\n",
    "    else:\n",
    "        model_name = \"Data2/MLMs/mlm_\" + str(fixed)\n",
    "        model = keras.models.load_model(model_name + '.h5')\n",
    "        all_preds = model.predict(test_text)\n",
    "        all_gofs = gof_spear(all_preds,test_labels)\n",
    "    return all_gofs,all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Per-Schema RNNs\n",
    "> We also train separate RNNs per schema. For this, we can use the output layer to compute a probability for each of the four possible labels. This way, the labels are treated as separate classes. We take over the parameter values from the multilabel model for the number of LSTM units, the dropout rate, the loss function, the evaluation metric, the batch size, and the number of epochs. To obtain the probability for each class, the units of the output layer have a softmax activation function. For the evaluation, the class with the highest probability is chosen per model. The resulting models are written to files and loaded again for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define separate models\n",
    "def perschema_models(train_X, train_y, test_X, test_y):\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "    model.add(e)\n",
    "    model.add(Bidirectional(LSTM(100)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['mean_absolute_error'])\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    # fit the model\n",
    "    model.fit(train_X, train_y,\n",
    "              validation_data=[test_X,test_y],\n",
    "              batch_size=32, \n",
    "              epochs=100, \n",
    "              verbose=0)\n",
    "    out=model.predict(test_X)\n",
    "    gof,p=scipy.stats.spearmanr(out,test_y,axis=None)\n",
    "    return gof, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load single models\n",
    "def load_single_models(directory):\n",
    "    single_models = []\n",
    "    for i in range(9):\n",
    "        model_name ='/schema_model_' + schemas[i]\n",
    "        get_from = directory + model_name\n",
    "        model = keras.models.load_model(get_from + '.h5')\n",
    "        single_models.append(model)\n",
    "    return single_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate predictions with the per-schema models\n",
    "def predict_schema_psm(test_text, test_labels,fixed=None):\n",
    "    if fixed is None:\n",
    "        all_preds = np.zeros((test_labels.shape[0],test_labels.shape[1],30)) \n",
    "        all_gofs = np.zeros((30,9)) \n",
    "        for j in range(30): \n",
    "            directory_name = \"Data2/PSMs/per_schema_models_\" + str(j)\n",
    "            preds = np.zeros(test_labels.shape)\n",
    "            gofs=[]\n",
    "            single_models = load_single_models(directory_name)\n",
    "            for i in range(9):\n",
    "                model = single_models[i]\n",
    "                out = model.predict(test_text)\n",
    "                out = out.argmax(axis=1)\n",
    "                preds[:,i] = out\n",
    "                gof,p=scipy.stats.spearmanr(out,test_labels[:,i])\n",
    "                gofs.append(gof)\n",
    "            all_preds[:,:,j] = preds\n",
    "            all_gofs[j,:] = gofs\n",
    "    else:\n",
    "        directory_name= \"Data2/PSMs/per_schema_models_\" + str(fixed)\n",
    "        all_preds = np.zeros(test_labels.shape)\n",
    "        all_gofs = []\n",
    "        single_models = load_single_models(directory_name)\n",
    "        for i in range(9):\n",
    "            model = single_models[i]\n",
    "            out = model.predict(test_text)\n",
    "            out = out.argmax(axis=1)\n",
    "            all_preds[:,i] = out\n",
    "            gof,p=scipy.stats.spearmanr(out,test_labels[:,i])\n",
    "            all_gofs.append(gof)\n",
    "    return all_gofs,all_preds    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Testset Predictions with the RNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_rnn(test_X,test_y,single):\n",
    "    if single:\n",
    "        gof,preds=predict_schema_psm(test_X,test_y)\n",
    "    else:\n",
    "        gof,preds=predict_schema_mlm(test_X,test_y)\n",
    "    \n",
    "    #make a sum of all classification values\n",
    "    gof_sum = np.sum(gof,axis=1)\n",
    "    #sort sums\n",
    "    gof_sum_sorted = np.sort(gof_sum)\n",
    "    #pick element that is closest but larger than median (we have even number of elements)\n",
    "    get_med_element = gof_sum_sorted[15] \n",
    "    #get index of median\n",
    "    gof_sum_med_idx = np.where(gof_sum==get_med_element)[0]\n",
    "    #choose this as the final model to use in H2 and to report in the paper\n",
    "    gof_out = gof[gof_sum_med_idx]\n",
    "    return np.transpose(gof_out),gof_sum_med_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 31s, sys: 40.6 s, total: 12min 12s\n",
      "Wall time: 9min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 6min\n",
    "# predicting testset with multilabel model\n",
    "output_mlm,idx_mlm = my_rnn(padded_test,test_labels,0)\n",
    "# predicting testset with perschema models\n",
    "output_psm,idx_psm = my_rnn(padded_test,test_labels,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Multilabel Model Testset Output\n",
      "          estimate\n",
      "Attach    0.686899\n",
      "Comp      0.662779\n",
      "Global    0.487473\n",
      "Health    0.351675\n",
      "Control   0.314296\n",
      "MetaCog   0.108215\n",
      "Others    0.158427\n",
      "Hopeless  0.533973\n",
      "OthViews  0.504708\n"
     ]
    }
   ],
   "source": [
    "print('RNN Multilabel Model Testset Output')\n",
    "print(pd.DataFrame(data=output_mlm,index=schemas,columns=['estimate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.72713769],\n",
       "       [ 0.75510694],\n",
       "       [ 0.57793988],\n",
       "       [ 0.7528724 ],\n",
       "       [ 0.27879326],\n",
       "       [-0.01286433],\n",
       "       [ 0.22367449],\n",
       "       [ 0.62756154],\n",
       "       [ 0.57866759]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_psm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Per-Schema Testset Output\n",
      "          estimate\n",
      "Attach    0.727138\n",
      "Comp      0.755107\n",
      "Global    0.577940\n",
      "Health    0.752872\n",
      "Control   0.278793\n",
      "MetaCog  -0.012864\n",
      "Others    0.223674\n",
      "Hopeless  0.627562\n",
      "OthViews  0.578668\n"
     ]
    }
   ],
   "source": [
    "print('RNN Per-Schema Testset Output')\n",
    "print(pd.DataFrame(data=output_psm,index=schemas,columns=['estimate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_rnn_fixed(test_X,test_y,single):\n",
    "    if single:\n",
    "        gof,preds=predict_schema_psm(test_X,test_y,idx_psm[0])\n",
    "    else:\n",
    "        gof,preds=predict_schema_mlm(test_X,test_y,idx_mlm[0])\n",
    "    return gof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_psm_flat = [item for sublist in output_psm for item in sublist]\n",
    "output_mlm_flat = [item for sublist in output_mlm for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimates of all models\n",
      "          kNN_class   kNN_reg       SVC       SVR       PSM       MLM\n",
      "Attach     0.550705  0.626743  0.647714  0.675340  0.727138  0.686899\n",
      "Comp       0.690230  0.663091  0.684661  0.640866  0.755107  0.662779\n",
      "Global     0.401123  0.411444  0.357601  0.489372  0.577940  0.487473\n",
      "Health     0.742217  0.534902  0.729181  0.349064  0.752872  0.351675\n",
      "Control    0.107526  0.231541       NaN  0.310007  0.278793  0.314296\n",
      "MetaCog         NaN  0.104785       NaN  0.114894 -0.012864  0.108215\n",
      "Others     0.279105  0.243713       NaN  0.185827  0.223674  0.158427\n",
      "Hopeless   0.484137  0.513825  0.489903  0.535979  0.627562  0.533973\n",
      "OthViews   0.454565  0.458473  0.476297  0.516635  0.578668  0.504708\n"
     ]
    }
   ],
   "source": [
    "print(f'Estimates of all models')\n",
    "outputs = np.concatenate((output_kNN_class,output_kNN_reg,output_SVC, output_SVR, output_psm_flat, output_mlm_flat))\n",
    "outputs=np.reshape(outputs,(9,6),order='F')\n",
    "print(pd.DataFrame(data=outputs,index=schemas,columns=['kNN_class','kNN_reg','SVC','SVR','PSM','MLM']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping for confidence intervals\n",
    "<br>\n",
    "Since all models are expensive to run, we only do a small bootstrapping to obtain some insight into how confident we can be about the predictions.\n",
    "<br>\n",
    "<br>\n",
    "******* Placing this section in the end since it is a bit time consuming to compute. Takes 3+ hours to complete. *******"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we adopt the algorithm from the following website:\n",
    "# https://machinelearningmastery.com/calculate-bootstrap-confidence-intervals-machine-learning-results-python/\n",
    "# def bootstrap\n",
    "def bootstrap(iterations,sample_size, sample_embeds, sample_labels,classification,model):\n",
    "    stats = np.zeros((iterations,9))\n",
    "    for l in range(iterations):\n",
    "        # prepare bootstrap sample\n",
    "        bootstrap_sample_indx = random.sample(list(enumerate(sample_embeds)), sample_size)\n",
    "        bootstrap_sample_utts = [sample_embeds[i] for (i,j) in bootstrap_sample_indx]\n",
    "        bootstrap_sample_labels = [sample_labels[i] for (i,j) in bootstrap_sample_indx]\n",
    "        # evaluate model\n",
    "        if model==\"knn\":\n",
    "            model_gof=my_kNN(np.array(bootstrap_sample_utts),np.array(bootstrap_sample_labels),classification)\n",
    "        elif model==\"svm\":\n",
    "            model_gof=my_svm(np.array(bootstrap_sample_utts),np.array(bootstrap_sample_labels),classification)\n",
    "        elif model==\"rnn\":\n",
    "            model_gof=my_rnn_fixed(np.array(bootstrap_sample_utts),np.array(bootstrap_sample_labels),classification)\n",
    "        stats[l,:] = model_gof\n",
    "    # confidence intervals\n",
    "    cis = np.zeros((2,9))\n",
    "    alpha = 0.95\n",
    "    p = ((1.0-alpha)/2.0) * 100\n",
    "    cis[0,:] = [max(0.0, np.percentile(stats[:,i], p)) for i in range(9)]\n",
    "    p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "    cis[1,:] = [min(1.0, np.percentile(stats[:,i], p)) for i in range(9)]\n",
    "    return cis\n",
    "\n",
    "# configure bootstrap\n",
    "n_iterations = 100\n",
    "n_size = int(len(val_text) * 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating confidence intervals for KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 3h 30min\n",
    "# bootstrap confidence intervals for kNN regression and classification\n",
    "bs_knn_reg = bootstrap(n_iterations,n_size,test_embedutts,test_labels,0,\"knn\")\n",
    "bs_knn_class = bootstrap(n_iterations,n_size,test_embedutts,test_labels,1,\"knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'KNN Regression 95% Confidence Intervals')\n",
    "print(pd.DataFrame(data=np.transpose(bs_knn_reg),index=schemas,columns=['low','high']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'KNN Classification 95% Confidence Intervals')\n",
    "print(pd.DataFrame(data=np.transpose(bs_knn_class),index=schemas,columns=['low','high']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating confidence intervals for SVM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 3min 15sec\n",
    "# bootstrap confidence intervals for SVR and SVC\n",
    "bs_svc = bootstrap(n_iterations,n_size,test_embedutts,test_labels,1,\"svm\")\n",
    "bs_svr = bootstrap(n_iterations,n_size,test_embedutts,test_labels,0,\"svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'SVM Classification 95% Confidence Intervals')\n",
    "print(pd.DataFrame(data=np.transpose(bs_svc),index=schemas,columns=['low','high']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'SVM Regression 95% Confidence Intervals')\n",
    "print(pd.DataFrame(data=np.transpose(bs_svr),index=schemas,columns=['low','high']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating confidence intervals for RNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 37min\n",
    "#bootstrapping the 95% confidence intervals\n",
    "bs_mlm = bootstrap(n_iterations,n_size,padded_test,test_labels,0,\"rnn\")\n",
    "bs_psm = bootstrap(n_iterations,n_size,padded_test,test_labels,1,\"rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Multilabel RNN Classification 95% Confidence Intervals')\n",
    "print(pd.DataFrame(data=np.transpose(bs_mlm),index=schemas,columns=['low','high']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Per-Schema RNN Classification 95% Confidence Intervals')\n",
    "print(pd.DataFrame(data=np.transpose(bs_psm),index=schemas,columns=['low','high']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing condidence intervals for all three algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Lower CIs of all models')\n",
    "lower_cis = np.concatenate((bs_knn_class[0],bs_knn_reg[0],bs_svc[0], bs_svr[0], bs_psm[0], bs_mlm[0]))\n",
    "lower_cis=np.reshape(lower_cis,(9,6),order='F')\n",
    "print(pd.DataFrame(data=lower_cis,index=schemas,columns=['kNN_class','kNN_reg','SVC','SVR','PSM','MLM']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Upper CIs of all models')\n",
    "upper_cis = np.concatenate((bs_knn_class[1],bs_knn_reg[1],bs_svc[1], bs_svr[1], bs_psm[1], bs_mlm[1]))\n",
    "upper_cis=np.reshape(upper_cis,(9,6),order='F')\n",
    "print(pd.DataFrame(data=upper_cis,index=schemas,columns=['kNN_class','kNN_reg','SVC','SVR','PSM','MLM']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3env",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
