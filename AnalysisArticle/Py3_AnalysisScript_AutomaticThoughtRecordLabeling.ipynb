{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python3 Part of the Data Analysis for the Journal Article Titled:\n",
    "# <i>Natural language processing for cognitive behavioral therapy: extracting schemas from thought records</i>\n",
    "\n",
    ">This script accompanies the journal article with the title stated above. The main aim of the research is to determine whether an algorithm can label utterances expressed in thought records with regard to the schema(s) they reflect. Thought record forms are a tool in cognitive therapy with which patients should gain insight into their maladaptive thought processes. According to the theory underlying cognitive therapy, it is these malaptive thought processes that result in the respective mental illness.  \n",
    "This script complements an R/KnitR script that consists of the following sections:\n",
    "    <ol> \n",
    "    <li>Preparing data for testing Hypothesis 1</li>\n",
    "    <li>Testing Hypothesis 2</li>\n",
    "    <li>Testing Hypothesis 3</li>\n",
    "    <li>Testing Hypothesis 4</li>\n",
    "    </ol>\n",
    "Details concerning the hypotheses, the project background, the raw data, and the data collection process can all be found in the R/KnitR script.<br>\n",
    "<br>\n",
    "The modules below need to be installed before running the code:\n",
    "    <ol>\n",
    "    <li>gensim==3.8.3</li>\n",
    "    <li>talos==0.6.3</li>\n",
    "    <li>tensorflow==2.3.2</li>\n",
    "    <li>statsmodels==0.10.2</li>\n",
    "    <li>scipy==1.4.1</li>\n",
    "    <li>scikit-learn==0.23.2</li>\n",
    "    <li>numpy==1.16.3</li>\n",
    "    <li>pandas==0.25.3</li>\n",
    "    </ol>\n",
    "<br>\n",
    "The following inputs are required and can be found in the DataRepository/AnalysisArticle/Data directory:\n",
    "    <ol>\n",
    "    <li>glove.6B directory</li>\n",
    "    <li>DatasetsForH1 directory</li>\n",
    "    </ol>\n",
    "<br>\n",
    "Additionally, the following output is generated:\n",
    "    <ol>\n",
    "    <li>data_for_H2.csv file</li>\n",
    "    <li>per_schema_models directory</li>\n",
    "    </ol>\n",
    "with the latter containing all trained per-schema RNN models in .h5 file format.  \n",
    "<br>\n",
    "The purpose of this script is to test Hypothesis 1, i.e. to see whether an algorithm can attach the correct schema label to thought record utterances more often than would be expected by chance. A thought record utterance could reflect none, any one, or multiple of 9 possible schemas. Additionally, labels are not binary (does or does not reflect schema) but ordinal (0 - has nothing to do with schema, 1 - has a little bit to do with the schema, 2 - has to do with the schema, 3 - fits perfectly with the schema). <br>\n",
    "Utterances are in natural language format. It is therefore necessary to preprocess these pieces of text, which we do in R. We also split the entire raw dataset into training, validation and test sets. The test set is created by taking 15% of the raw data, the validation set is created by taking another 15% of the remaining data. <br>\n",
    "Three algorithms are explored: k-nearest neighbors, support vector machines, and recurrent neural networks. We arrived at the former two, by following the decision tree presented by scikitLearn (https://scikit-learn.org/stable/tutorial/machine_learning_map/). The data are ordinal, labeled, and we have less than 100k samples. The recurrent neural networks are a logical choice for natural language data, since they allow modelling the temporal aspect that is inherent to sentences as sequences of words.<br>\n",
    "The wall time of runtimes are provided in the first comment of cells of time intensive code. Additionally, the cell magic \"%%time\" in these cells ensures that runtimes are printed so that these can be compared to the reported runtimes to get appropriate estimates when running on a different machine.<br>\n",
    "We import the following packages and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#set seed\n",
    "seed = 57839\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed)\n",
    "import sys\n",
    "\n",
    "import random\n",
    "random.seed(seed)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed)\n",
    "\n",
    "import csv\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats as stats\n",
    "import functools\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn import metrics, preprocessing, svm\n",
    "from sklearn.utils import resample\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_random_seed(seed)\n",
    "\n",
    "from tensorflow.python.keras.metrics import Metric\n",
    "from tensorflow import keras\n",
    "import talos\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dense, Flatten, Embedding, SimpleRNN, LSTM, GRU, Bidirectional,Dropout\n",
    "\n",
    "from keras import backend as K\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.13 (default, Mar 28 2022, 07:24:34) \n",
      "[Clang 12.0.0 ]\n"
     ]
    }
   ],
   "source": [
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gensim==3.8.3\n",
      "numpy==1.21.6\n",
      "tensorflow==1.14.0\n",
      "scipy==1.4.1\n",
      "pandas==0.25.3\n",
      "scikit-learn==0.23.2\n",
      "statsmodels==0.13.1\n",
      "talos==0.6.3\n"
     ]
    }
   ],
   "source": [
    "#list packages and their version numbers as used in this script (code is taken from \n",
    "#https://stackoverflow.com/questions/40428931/package-for-listing-version-of-packages-used-in-a-jupyter-notebook)\n",
    "import pkg_resources\n",
    "import types\n",
    "def get_imports():\n",
    "    for name, val in globals().items():\n",
    "        if isinstance(val, types.ModuleType):\n",
    "            # Split ensures you get root package, \n",
    "            # not just imported function\n",
    "            name = val.__name__.split(\".\")[0]\n",
    "\n",
    "        elif isinstance(val, type):\n",
    "            name = val.__module__.split(\".\")[0]\n",
    "\n",
    "        # Some packages are weird and have different\n",
    "        # imported names vs. system/pip names. Unfortunately,\n",
    "        # there is no systematic way to get pip names from\n",
    "        # a package's imported name. You'll have to add\n",
    "        # exceptions to this list manually!\n",
    "        poorly_named_packages = {\n",
    "            \"sklearn\": \"scikit-learn\"\n",
    "        }\n",
    "        if name in poorly_named_packages.keys():\n",
    "            name = poorly_named_packages[name]\n",
    "\n",
    "        yield name\n",
    "imports = list(set(get_imports()))\n",
    "\n",
    "# The only way I found to get the version of the root package\n",
    "# from only the name of the package is to cross-check the names \n",
    "# of installed packages vs. imported packages\n",
    "requirements = []\n",
    "for m in pkg_resources.working_set:\n",
    "    if m.project_name in imports and m.project_name!=\"pip\":\n",
    "        requirements.append((m.project_name, m.version))\n",
    "\n",
    "for r in requirements:\n",
    "    print(\"{}=={}\".format(*r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We also set the working directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Volumes/ProjectData/UIUC/DLH/Project/DataRepository/AnalysisArticle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the datasets from csv\n",
    "> The preprocessed utterances are split into three sets in the R script. They are saved in three separate csv files. Additionally, the manually assigned labels that correspond with the utterances are saved in three separate csv files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in datasets (already pre-processed)\n",
    "def readcsv(fname,istext):\n",
    "    if istext:\n",
    "        with open(fname,'rt') as f:\n",
    "            reader=csv.reader(f)\n",
    "            next(reader)\n",
    "            data = []\n",
    "            for row in reader:\n",
    "                for item in row:\n",
    "                    data.append(item)\n",
    "            f.close()\n",
    "    else:\n",
    "        with open(fname,'r') as f:\n",
    "            reader=csv.reader(f,delimiter=';')\n",
    "            next(reader)\n",
    "            data = list(reader)\n",
    "            data = np.asarray(data, dtype='int')\n",
    "            f.close()\n",
    "    return data \n",
    "\n",
    "# read in training, validation, and test set utterances\n",
    "train_text = readcsv('Data/DatasetsForH1/H1_train_texts.csv',True)\n",
    "val_text = readcsv('Data/DatasetsForH1/H1_validate_texts.csv', True)\n",
    "test_text = readcsv('Data/DatasetsForH1/H1_test_texts.csv',True)\n",
    "\n",
    "# read in training, validation, and test set labels\n",
    "train_labels = readcsv('Data/DatasetsForH1/H1_train_labels.csv',False)[:,0:9]\n",
    "val_labels = readcsv('Data/DatasetsForH1/H1_validate_labels.csv', False)[:,0:9]\n",
    "test_labels = readcsv('Data/DatasetsForH1/H1_test_labels.csv',False)[:,0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lot people may think well lot people might not like me', 'might not working fast enough their standards', 'may not able graduate', 'would get bad performance review', 'friends will get annoyed by me']\n"
     ]
    }
   ],
   "source": [
    "print(train_text[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 0 0 0 0 0 0 0 3]\n",
      " [0 3 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0]\n",
      " [2 0 0 0 0 0 0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "print(train_labels[0:5,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> As can be seen, some utterances have multiple schemas assigned. However, overall, the label matrices are sparse matrices. The first column of the labels corresponds to the \"Attachment\" schema, the second to the \"Competence\" schema, the third to last to the \"Other's views on self\" schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for later use\n",
    "schemas = [\"Attach\",\"Comp\",\"Global\",\"Health\",\"Control\",\"MetaCog\",\"Others\",\"Hopeless\",\"OthViews\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding the utterances using GLoVE\n",
    "> We have opted for representing the words in utterances as word vectors. We adopt the GLoVE word vector space that has been created with Wikipedia 2014. First, we tokenize the top 2000 words of the training set.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2624\n"
     ]
    }
   ],
   "source": [
    "# prepare tokenizer\n",
    "max_words = 2000\n",
    "t = Tokenizer(num_words = max_words)\n",
    "t.fit_on_texts(train_text)\n",
    "vocab_size = len(t.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The tokenizer takes the words and indexes these based on frequency. For the recurrent neural net, we need padded utterances sequences. Texts_to_sequences simply represents each utterance as a vector of tokens. Padding ensures that all vectors are of the same length, by appending 0s to the end of shorter vectors. We pad to a length of 25 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[147, 28, 48, 37, 101, 147, 28, 32, 1, 8, 5], [32, 1, 155, 658, 14, 125, 568], [48, 1, 19, 448], [2, 11, 53, 449, 659], [50, 6, 11, 373, 98, 5]]\n"
     ]
    }
   ],
   "source": [
    "# integer encode all utterances\n",
    "encoded_train = t.texts_to_sequences(train_text)\n",
    "encoded_validate = t.texts_to_sequences(val_text)\n",
    "encoded_test = t.texts_to_sequences(test_text)\n",
    "\n",
    "# pad documents to a max length of 25 words\n",
    "max_length = 25\n",
    "\n",
    "padded_train = pad_sequences(encoded_train, maxlen=max_length, padding='post')\n",
    "padded_validate = pad_sequences(encoded_validate, maxlen=max_length, padding='post')\n",
    "padded_test = pad_sequences(encoded_test, maxlen=max_length, padding='post')\n",
    "\n",
    "print(encoded_train[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[147  28  48  37 101 147  28  32   1   8   5   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 32   1 155 658  14 125 568   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 48   1  19 448   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [  2  11  53 449 659   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]\n",
      " [ 50   6  11 373  98   5   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   0   0   0   0]]\n"
     ]
    }
   ],
   "source": [
    "print(padded_train[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can now load the GLoVE embeddings into memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n",
      "CPU times: user 8.22 s, sys: 307 ms, total: 8.53 s\n",
      "Wall time: 8.62 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 10sec\n",
    "# load all embeddings into memory\n",
    "embeddings_index = dict()\n",
    "f = open('Data/glove.6B/glove.6B.100d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We can then create an embedding matrix by taking each word of the training set and finding the corresponding word vector in the GLoVE data. We only work with 100 dimensional representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_dims = 100\n",
    "embedding_matrix = np.zeros((vocab_size, vec_dims))\n",
    "for word, i in t.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.29214445 0.         ... 0.         0.         0.        ]\n",
      " [0.         1.29214445 0.         ... 0.         0.         0.        ]\n",
      " [0.         1.29214445 0.         ... 0.         0.         0.        ]\n",
      " [0.         0.         1.69021763 ... 0.         0.         0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.        ]]\n",
      "(4151, 2000)\n"
     ]
    }
   ],
   "source": [
    "# create tfidf weighted encoding matrix of utterances\n",
    "train_sequences = t.texts_to_matrix(train_text,mode='tfidf')\n",
    "val_sequences =  t.texts_to_matrix(val_text,mode='tfidf')\n",
    "test_sequences = t.texts_to_matrix(test_text,mode='tfidf')\n",
    "print(train_sequences[0:5])\n",
    "print(train_sequences.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to normalize the word vectors\n",
    "def normalize(v):\n",
    "    norm = np.linalg.norm(v)\n",
    "    if norm == 0: \n",
    "       return v\n",
    "    return v / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create utterance embeddings as tfidf weighted average of normalized word vectors\n",
    "def seq2vec(datarow,embedmat):\n",
    "  #initialize an empty utterance vector of the same length as word2vec vectors\n",
    "  seqvec = np.zeros((100,))\n",
    "  #counter for number of words in a specific utterance\n",
    "  wordcount = 1\n",
    "  #we iterate over the 2000 possible words in a given utterance\n",
    "  wordind = 1\n",
    "  while (wordind < len(datarow)):\n",
    "    #the tf-idf weight is saved in the cells of datarow\n",
    "    tfidfweight = datarow[wordind]\n",
    "    if not tfidfweight is None:\n",
    "      wordembed = tfidfweight * embedmat[wordind,]\n",
    "      seqvec = seqvec + normalize(wordembed)\n",
    "      wordcount = wordcount + 1\n",
    "    wordind = wordind + 1\n",
    "  return seqvec/wordcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go through the matrix and embed each utterances\n",
    "def embed_utts(sequences,embedmat):\n",
    "  vecseq = [seq2vec(seq,embedmat)for seq in sequences]\n",
    "  return vecseq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> we now have everything needed to create the utterance embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-3.44478099e-05  3.48539840e-04  3.35962753e-04 -3.70855457e-04\n",
      " -2.63147438e-04  1.07227074e-04 -1.66559109e-04  1.94234500e-05\n",
      "  7.42420570e-05 -1.80615841e-04  1.80387578e-05  4.92242034e-05\n",
      "  2.75006568e-04  2.34416192e-05  8.31148165e-05 -2.93833280e-04\n",
      " -7.15121389e-05  2.98592314e-04 -4.55134987e-04  4.72657153e-04\n",
      "  2.57585086e-04  1.69741478e-04  7.75960265e-05 -2.15817394e-04\n",
      " -4.34789085e-05  7.24571212e-05 -1.54585404e-04 -4.98166781e-04\n",
      "  1.93941088e-04 -1.74921206e-04  2.37557331e-05  4.85150809e-04\n",
      "  3.08554881e-05 -4.62293641e-05  1.35110613e-04  2.80284189e-04\n",
      " -3.22980711e-05  3.12968134e-04  8.27704500e-05 -2.40951546e-04\n",
      " -3.13527886e-04 -1.35440392e-04 -2.05195768e-05 -4.81099111e-04\n",
      " -2.75375333e-04 -1.27601856e-04  2.50256011e-04 -2.50631136e-04\n",
      " -1.51297680e-04 -8.33555219e-04  3.54382525e-05 -8.74190709e-05\n",
      "  1.05239327e-05  8.00132559e-04 -1.52039351e-04 -1.90058573e-03\n",
      "  9.49153013e-05 -1.17238522e-05  1.18845110e-03  3.93093667e-04\n",
      " -1.88908628e-04  9.94003983e-04 -3.57759952e-04  2.93419204e-05\n",
      "  6.71172261e-04  1.52662986e-04  5.76767087e-04  2.42848249e-04\n",
      "  9.63268891e-06 -2.31785203e-04  1.51988867e-05 -2.98388563e-04\n",
      " -3.74962639e-05 -3.26247156e-04  6.07764018e-05  6.39454626e-05\n",
      "  8.71421849e-05  6.76090198e-05 -5.16919711e-04 -2.17311368e-05\n",
      "  4.80646118e-04 -1.64901023e-04 -5.06686209e-04  1.66221153e-05\n",
      " -1.31359098e-03 -2.02032865e-04 -2.72980358e-05 -6.66373277e-05\n",
      " -3.30798167e-04 -3.81777162e-04 -2.22552903e-05 -2.22256701e-04\n",
      "  4.71648347e-05 -1.49116944e-04 -4.30563119e-04  4.04963585e-05\n",
      " -2.00736932e-04 -3.13451294e-04  2.94196617e-04  2.83769604e-04]\n",
      "CPU times: user 1min 33s, sys: 1.57 s, total: 1min 34s\n",
      "Wall time: 1min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 1min 14s\n",
    "# embedd all three datasets\n",
    "train_embedutts = embed_utts(train_sequences,embedding_matrix)\n",
    "val_embedutts = embed_utts(val_sequences,embedding_matrix)\n",
    "test_embedutts = embed_utts(test_sequences,embedding_matrix)\n",
    "print(train_embedutts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "> We use the Spearman correlation to evaluate the models and choose the best one, because it can be used for both the regression and the classification outcomes. This is not the case for a weighted Cohen's Kappa, for example, which only works for class labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Goodness of Fit\n",
    "def gof_spear(X,Y):\n",
    "    #spearman correlation of columns (schemas)\n",
    "    gof_spear = np.zeros(X.shape[1])    \n",
    "    for schema in range(9):\n",
    "        rho,p = scipy.stats.spearmanr(X[:,schema],Y[:,schema])\n",
    "        gof_spear[schema]=rho\n",
    "    return gof_spear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrapping for confidence intervals\n",
    "> Since all models are expensive to run, we only do a small bootstrapping to obtain some insight into how confident we can be about the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we adopt the algorithm from the following website:\n",
    "# https://machinelearningmastery.com/calculate-bootstrap-confidence-intervals-machine-learning-results-python/\n",
    "# def bootstrap\n",
    "def bootstrap(iterations,sample_size, sample_embeds, sample_labels,classification,model):\n",
    "    stats = np.zeros((iterations,9))\n",
    "    for l in range(iterations):\n",
    "        # prepare bootstrap sample\n",
    "        bootstrap_sample_indx = random.sample(list(enumerate(sample_embeds)), sample_size)\n",
    "        bootstrap_sample_utts = [sample_embeds[i] for (i,j) in bootstrap_sample_indx]\n",
    "        bootstrap_sample_labels = [sample_labels[i] for (i,j) in bootstrap_sample_indx]\n",
    "        # evaluate model\n",
    "        if model==\"knn\":\n",
    "            model_gof=my_kNN(np.array(bootstrap_sample_utts),np.array(bootstrap_sample_labels),classification)\n",
    "        elif model==\"svm\":\n",
    "            model_gof=my_svm(np.array(bootstrap_sample_utts),np.array(bootstrap_sample_labels),classification)\n",
    "        elif model==\"rnn\":\n",
    "            model_gof=my_rnn_fixed(np.array(bootstrap_sample_utts),np.array(bootstrap_sample_labels),classification)\n",
    "        stats[l,:] = model_gof\n",
    "    # confidence intervals\n",
    "    cis = np.zeros((2,9))\n",
    "    alpha = 0.95\n",
    "    p = ((1.0-alpha)/2.0) * 100\n",
    "    cis[0,:] = [max(0.0, np.percentile(stats[:,i], p)) for i in range(9)]\n",
    "    p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "    cis[1,:] = [min(1.0, np.percentile(stats[:,i], p)) for i in range(9)]\n",
    "    return cis\n",
    "\n",
    "# configure bootstrap\n",
    "n_iterations = 100\n",
    "n_size = int(len(val_text) * 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-nearest Neighbors Classification and Regression\n",
    "> Since we have ordinal labels for our data, we train both classification and regression algorithms and see which one performs better. We also have multi-label data, and therefore write a custom kNN algorithm. We use the cosine distance to find the nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine distance\n",
    "def cosine_dist(X,Y):\n",
    "    return scipy.spatial.distance.cosine(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kNN algorithm\n",
    "def knn_custom(train_X,test_X,train_y,test_y,k,dist,classification):\n",
    "    #empty array to collect the results (should have shape of samples to classify)\n",
    "    votes = np.zeros(test_y.shape)\n",
    "    #fit the knn\n",
    "    knn=NearestNeighbors(k, metric=dist)\n",
    "    knn.fit(train_X)\n",
    "    #collect neighbors\n",
    "    i=0 # index to collect votes of the neighbors\n",
    "    for sample in test_X:\n",
    "        neighbors=knn.kneighbors([sample],k,return_distance=False)[0]\n",
    "        if classification:\n",
    "            output_y = np.zeros((k,test_y.shape[1]))\n",
    "            j=0\n",
    "            for neighbor in neighbors:\n",
    "                output_y[j,:] = train_y[neighbor,:]\n",
    "                j=j+1\n",
    "            votes[i,:] = stats.mode(output_y,nan_policy='omit')[0]\n",
    "        else:\n",
    "            output_y = np.zeros(test_y.shape[1])\n",
    "            for neighbor in neighbors:\n",
    "                output_y += train_y[neighbor,:]\n",
    "                votes[i,:] = np.divide(output_y,k)\n",
    "        i=i+1\n",
    "    return votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> To evaluate choices for k, we use a performance metric that is a weighted mean of the spearman correlation for each choice of k. As weights we use the frequencies of schemas (# of utterances with labels > 0 for a given schema/total number of utterances) in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighting model output (spearman correlations) by schema frequencies in training set and returning mean over schemas\n",
    "def performance(train_y,output):\n",
    "    train_y = np.array(train_y)\n",
    "    train_y[train_y>0]=1\n",
    "    weighting = train_y.sum(axis=0)/train_y.shape[0]\n",
    "    perf = output * weighting\n",
    "    return np.nanmean(np.array(perf), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding best k by testing some values for k\n",
    "def find_k(train_X, test_X, train_y, test_y, dist, classification):\n",
    "    perf = 0\n",
    "    best_k = 0\n",
    "    for k in [2,3,4,5,6,7,8,9,10,30,100]:\n",
    "        knn_k = knn_custom(train_X, test_X, train_y, test_y,k,dist,classification)\n",
    "        knn_gof_spear = gof_spear(knn_k,test_y)\n",
    "        print('Results for choice of k is %s.' % k)\n",
    "        print(pd.DataFrame(data=knn_gof_spear,index=schemas,columns=['gof']))\n",
    "        if perf < performance(train_y,knn_gof_spear):\n",
    "            perf = performance(train_y,knn_gof_spear)\n",
    "            best_k = k\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for choice of k is 2.\n",
      "               gof\n",
      "Attach    0.535011\n",
      "Comp      0.645117\n",
      "Global    0.549926\n",
      "Health    0.593794\n",
      "Control   0.132613\n",
      "MetaCog  -0.005695\n",
      "Others   -0.006347\n",
      "Hopeless  0.477770\n",
      "OthViews  0.414780\n",
      "Results for choice of k is 3.\n",
      "               gof\n",
      "Attach    0.540542\n",
      "Comp      0.648290\n",
      "Global    0.575273\n",
      "Health    0.614754\n",
      "Control   0.096685\n",
      "MetaCog        NaN\n",
      "Others   -0.008982\n",
      "Hopeless  0.559148\n",
      "OthViews  0.438930\n",
      "Results for choice of k is 4.\n",
      "               gof\n",
      "Attach    0.608176\n",
      "Comp      0.651848\n",
      "Global    0.556168\n",
      "Health    0.608395\n",
      "Control   0.185125\n",
      "MetaCog        NaN\n",
      "Others         NaN\n",
      "Hopeless  0.503857\n",
      "OthViews  0.492713\n",
      "Results for choice of k is 5.\n",
      "               gof\n",
      "Attach    0.585641\n",
      "Comp      0.631078\n",
      "Global    0.577591\n",
      "Health    0.645793\n",
      "Control   0.199890\n",
      "MetaCog        NaN\n",
      "Others         NaN\n",
      "Hopeless  0.543466\n",
      "OthViews  0.463903\n",
      "Results for choice of k is 6.\n",
      "               gof\n",
      "Attach    0.602287\n",
      "Comp      0.619769\n",
      "Global    0.547025\n",
      "Health    0.625649\n",
      "Control   0.143109\n",
      "MetaCog        NaN\n",
      "Others         NaN\n",
      "Hopeless  0.489336\n",
      "OthViews  0.471496\n",
      "Results for choice of k is 7.\n",
      "               gof\n",
      "Attach    0.610569\n",
      "Comp      0.625760\n",
      "Global    0.570156\n",
      "Health    0.645361\n",
      "Control   0.143109\n",
      "MetaCog        NaN\n",
      "Others         NaN\n",
      "Hopeless  0.498241\n",
      "OthViews  0.457757\n",
      "Results for choice of k is 8.\n",
      "               gof\n",
      "Attach    0.602182\n",
      "Comp      0.630256\n",
      "Global    0.554859\n",
      "Health    0.586944\n",
      "Control   0.153534\n",
      "MetaCog        NaN\n",
      "Others         NaN\n",
      "Hopeless  0.493097\n",
      "OthViews  0.430551\n",
      "Results for choice of k is 9.\n",
      "               gof\n",
      "Attach    0.592383\n",
      "Comp      0.629953\n",
      "Global    0.571065\n",
      "Health    0.607722\n",
      "Control   0.179243\n",
      "MetaCog        NaN\n",
      "Others         NaN\n",
      "Hopeless  0.498153\n",
      "OthViews  0.435328\n",
      "Results for choice of k is 10.\n",
      "               gof\n",
      "Attach    0.608948\n",
      "Comp      0.622814\n",
      "Global    0.570359\n",
      "Health    0.607722\n",
      "Control   0.179243\n",
      "MetaCog        NaN\n",
      "Others         NaN\n",
      "Hopeless  0.473538\n",
      "OthViews  0.385825\n",
      "Results for choice of k is 30.\n",
      "               gof\n",
      "Attach    0.545306\n",
      "Comp      0.582767\n",
      "Global    0.519405\n",
      "Health    0.438394\n",
      "Control        NaN\n",
      "MetaCog        NaN\n",
      "Others         NaN\n",
      "Hopeless  0.380605\n",
      "OthViews  0.409208\n",
      "Results for choice of k is 100.\n",
      "               gof\n",
      "Attach    0.371825\n",
      "Comp      0.565641\n",
      "Global    0.240219\n",
      "Health         NaN\n",
      "Control        NaN\n",
      "MetaCog        NaN\n",
      "Others         NaN\n",
      "Hopeless  0.194090\n",
      "OthViews  0.367514\n",
      "CPU times: user 18min 56s, sys: 15.5 s, total: 19min 11s\n",
      "Wall time: 21min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 15min\n",
    "# find best k for classification\n",
    "knn_class_k = find_k(train_embedutts,val_embedutts,train_labels,val_labels,cosine_dist,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best choice for classification k is: 4\n"
     ]
    }
   ],
   "source": [
    "print('Best choice for classification k is: %s' % knn_class_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for choice of k is 2.\n",
      "               gof\n",
      "Attach    0.576639\n",
      "Comp      0.655949\n",
      "Global    0.479923\n",
      "Health    0.590129\n",
      "Control   0.219795\n",
      "MetaCog  -0.031264\n",
      "Others    0.063275\n",
      "Hopeless  0.527345\n",
      "OthViews  0.465825\n",
      "Results for choice of k is 3.\n",
      "               gof\n",
      "Attach    0.590981\n",
      "Comp      0.646035\n",
      "Global    0.479578\n",
      "Health    0.559802\n",
      "Control   0.215631\n",
      "MetaCog   0.032108\n",
      "Others    0.154095\n",
      "Hopeless  0.498075\n",
      "OthViews  0.471136\n",
      "Results for choice of k is 4.\n",
      "               gof\n",
      "Attach    0.590042\n",
      "Comp      0.648248\n",
      "Global    0.481624\n",
      "Health    0.547869\n",
      "Control   0.242575\n",
      "MetaCog   0.040388\n",
      "Others    0.162686\n",
      "Hopeless  0.479595\n",
      "OthViews  0.468530\n",
      "Results for choice of k is 5.\n",
      "               gof\n",
      "Attach    0.610950\n",
      "Comp      0.665623\n",
      "Global    0.480601\n",
      "Health    0.535627\n",
      "Control   0.250752\n",
      "MetaCog   0.085472\n",
      "Others    0.182273\n",
      "Hopeless  0.488905\n",
      "OthViews  0.459507\n",
      "Results for choice of k is 6.\n",
      "               gof\n",
      "Attach    0.614144\n",
      "Comp      0.662029\n",
      "Global    0.477562\n",
      "Health    0.491082\n",
      "Control   0.241846\n",
      "MetaCog   0.099125\n",
      "Others    0.211419\n",
      "Hopeless  0.489645\n",
      "OthViews  0.457805\n",
      "Results for choice of k is 7.\n",
      "               gof\n",
      "Attach    0.610199\n",
      "Comp      0.650234\n",
      "Global    0.483970\n",
      "Health    0.480363\n",
      "Control   0.257023\n",
      "MetaCog   0.087772\n",
      "Others    0.239854\n",
      "Hopeless  0.481519\n",
      "OthViews  0.454791\n",
      "Results for choice of k is 8.\n",
      "               gof\n",
      "Attach    0.609293\n",
      "Comp      0.652533\n",
      "Global    0.480621\n",
      "Health    0.466533\n",
      "Control   0.261509\n",
      "MetaCog   0.094351\n",
      "Others    0.226930\n",
      "Hopeless  0.494483\n",
      "OthViews  0.452129\n",
      "Results for choice of k is 9.\n",
      "               gof\n",
      "Attach    0.609712\n",
      "Comp      0.653952\n",
      "Global    0.499796\n",
      "Health    0.465768\n",
      "Control   0.258580\n",
      "MetaCog   0.083452\n",
      "Others    0.224409\n",
      "Hopeless  0.489012\n",
      "OthViews  0.458946\n",
      "Results for choice of k is 10.\n",
      "               gof\n",
      "Attach    0.611310\n",
      "Comp      0.651579\n",
      "Global    0.495962\n",
      "Health    0.452848\n",
      "Control   0.265579\n",
      "MetaCog   0.072583\n",
      "Others    0.207547\n",
      "Hopeless  0.486279\n",
      "OthViews  0.457637\n",
      "Results for choice of k is 30.\n",
      "               gof\n",
      "Attach    0.602206\n",
      "Comp      0.626982\n",
      "Global    0.490143\n",
      "Health    0.366280\n",
      "Control   0.283329\n",
      "MetaCog   0.087035\n",
      "Others    0.150378\n",
      "Hopeless  0.440267\n",
      "OthViews  0.467081\n",
      "Results for choice of k is 100.\n",
      "               gof\n",
      "Attach    0.584510\n",
      "Comp      0.595527\n",
      "Global    0.470224\n",
      "Health    0.350984\n",
      "Control   0.267859\n",
      "MetaCog   0.062608\n",
      "Others    0.153848\n",
      "Hopeless  0.436157\n",
      "OthViews  0.475043\n",
      "CPU times: user 18min 40s, sys: 13.1 s, total: 18min 53s\n",
      "Wall time: 19min 54s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 15min\n",
    "# find best k for regression\n",
    "knn_reg_k = find_k(train_embedutts,val_embedutts,train_labels,val_labels,cosine_dist,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best choice for regression k is: 5\n"
     ]
    }
   ],
   "source": [
    "print('Best choice for regression k is: %s' % knn_reg_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since this is needed for the bootstrapping algorithm, we define a function that takes testset and labels and returns the goodness of fit. We print the results on the testset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_kNN(test_X,test_y,classification):\n",
    "    if classification:\n",
    "        my_knn=knn_custom(train_embedutts,test_X,train_labels,test_y,4,cosine_dist,1)\n",
    "    else:\n",
    "        my_knn=knn_custom(train_embedutts,test_X,train_labels,test_y,5,cosine_dist,0)\n",
    "    return gof_spear(my_knn,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 12s, sys: 3.11 s, total: 4min 15s\n",
      "Wall time: 4min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#wall time to run: ~ 4min\n",
    "output_kNN_class = my_kNN(test_embedutts,test_labels,1)\n",
    "output_kNN_reg = my_kNN(test_embedutts,test_labels,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Classification Prediction\n",
      "          estimate\n",
      "Attach    0.550705\n",
      "Comp      0.690230\n",
      "Global    0.401123\n",
      "Health    0.742217\n",
      "Control   0.107526\n",
      "MetaCog        NaN\n",
      "Others    0.279105\n",
      "Hopeless  0.484137\n",
      "OthViews  0.454565\n"
     ]
    }
   ],
   "source": [
    "print('KNN Classification Prediction')\n",
    "print(pd.DataFrame(data=output_kNN_class,index=schemas,columns=['estimate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Regression Prediction\n",
      "          estimate\n",
      "Attach    0.626743\n",
      "Comp      0.663091\n",
      "Global    0.411444\n",
      "Health    0.534902\n",
      "Control   0.231541\n",
      "MetaCog   0.104785\n",
      "Others    0.243713\n",
      "Hopeless  0.513825\n",
      "OthViews  0.458473\n"
     ]
    }
   ],
   "source": [
    "print('KNN Regression Prediction')\n",
    "print(pd.DataFrame(data=output_kNN_reg,index=schemas,columns=['estimate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 3h 30min\n",
    "# bootstrap confidence intervals for kNN regression and classification\n",
    "bs_knn_reg = bootstrap(n_iterations,n_size,test_embedutts,test_labels,0,\"knn\")\n",
    "bs_knn_class = bootstrap(n_iterations,n_size,test_embedutts,test_labels,1,\"knn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'KNN Classification 95% Confidence Intervals')\n",
    "print(pd.DataFrame(data=np.transpose(bs_knn_class),index=schemas,columns=['low','high']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'KNN Regression 95% Confidence Intervals')\n",
    "print(pd.DataFrame(data=np.transpose(bs_knn_reg),index=schemas,columns=['low','high']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support vector machine\n",
    "> The second algorithm we chose are support vector machines (SVMs). Again, we train both a support vector classification (SVC) and a support vectore regression (SVR). We only try all three types of standard kernels and do not do any additional parameter tuning. Just like the kNN, the support vector machine takes as input the utterances encoded as averages of word vectors. Support vector classification and regression do not allow for multilabel output. We therefore train disjoint models, one for each schema.<br>\n",
    "For both types of SVM, we first transform the input texts as the algorithm expects normally distributed input centered around 0 and with a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM/SVR\n",
    "def svm_scaler(train_X):\n",
    "        #scale the data\n",
    "        scaler_texts = StandardScaler()\n",
    "        scaler_texts = scaler_texts.fit(train_X)\n",
    "        return scaler_texts\n",
    "\n",
    "scaler_texts = svm_scaler(train_embedutts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Since SVMs, unlike kNNs, can be trained and reused, we write a method that returns all 9 models and a separate one for the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_custom(train_X,train_y,text_scaler,kern,classification):\n",
    "        models=[]\n",
    "        train_X = text_scaler.transform(train_X)\n",
    "        #fit a new support vector regression for each schema\n",
    "        for schema in range(9):\n",
    "            if classification:\n",
    "                model = svm.SVC(kernel=kern)\n",
    "            else:\n",
    "                model = svm.SVR(kernel=kern)\n",
    "            model.fit(train_X, train_y[:,schema])\n",
    "            models.append(model)\n",
    "        return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_predict(svm_models,test_X,train_y,test_y,text_scaler):\n",
    "    #empty array to collect the results (should have shape of samples to classify)\n",
    "    votes = np.zeros(test_y.shape)\n",
    "    for schema in range(9):\n",
    "        svm_model=svm_models[schema]\n",
    "        prediction = svm_model.predict(text_scaler.transform(test_X))\n",
    "        votes[:,schema] = prediction\n",
    "    out = votes\n",
    "    gof = gof_spear(out,test_y)\n",
    "    perf = performance(train_y,gof)\n",
    "    return out,perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 2min 20sec\n",
    "# svr\n",
    "svr_rbf_models =  svm_custom(train_embedutts,train_labels,scaler_texts,'rbf',0)\n",
    "svr_rbf_out, svr_rbf_perf = svm_predict(svr_rbf_models,val_embedutts,train_labels,val_labels,scaler_texts)\n",
    "svr_lin_models = svm_custom(train_embedutts,train_labels,scaler_texts,'linear',0)\n",
    "svr_lin_out, svr_lin_perf = svm_predict(svr_lin_models,val_embedutts,train_labels,val_labels,scaler_texts)\n",
    "svr_poly_models = svm_custom(train_embedutts,train_labels,scaler_texts,'poly',0)\n",
    "svr_poly_out, svr_poly_perf = svm_predict(svr_poly_models,val_embedutts,train_labels,val_labels,scaler_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(data=[svr_rbf_perf,svr_lin_perf,svr_poly_perf],index=['rbf','lin','poly'],columns=['svr']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 45sec\n",
    "# svm\n",
    "svm_rbf_models =  svm_custom(train_embedutts,train_labels,scaler_texts,'rbf',1)\n",
    "svm_rbf_out, svm_rbf_perf = svm_predict(svm_rbf_models,val_embedutts,train_labels,val_labels,scaler_texts)\n",
    "svm_lin_models = svm_custom(train_embedutts,train_labels,scaler_texts,'linear',1)\n",
    "svm_lin_out, svm_lin_perf = svm_predict(svm_lin_models,val_embedutts,train_labels,val_labels,scaler_texts)\n",
    "svm_poly_models = svm_custom(train_embedutts,train_labels,scaler_texts,'poly',1)\n",
    "svm_poly_out, svm_poly_perf = svm_predict(svm_poly_models,val_embedutts,train_labels,val_labels,scaler_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(data=[svm_rbf_perf,svm_lin_perf,svm_poly_perf],index=['rbf','lin','poly'],columns=['svm']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In both algorithms, the radial basis function (rbf) kernel outperformed linear and polynomial kernels. We therefore opt for the rbf kernel when predicting the labels of the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wall time to run: 4sec\n",
    "def my_svm(test_X,test_y,classification):\n",
    "    if classification:\n",
    "        my_svm_out, my_svm_perf=svm_predict(svm_rbf_models,test_X,train_labels,test_y,scaler_texts)\n",
    "    else:\n",
    "        my_svm_out, my_svm_perf=svm_predict(svr_rbf_models,test_X,train_labels,test_y,scaler_texts)\n",
    "    return gof_spear(my_svm_out,test_y)\n",
    "\n",
    "output_SVC = my_svm(test_embedutts,test_labels,1)\n",
    "output_SVR = my_svm(test_embedutts,test_labels,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM Classification Prediction')\n",
    "print(pd.DataFrame(data=output_SVC,index=schemas,columns=['estimate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVM Regression Prediction')\n",
    "print(pd.DataFrame(data=output_SVR,index=schemas,columns=['estimate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 3min 15sec\n",
    "# bootstrap confidence intervals for SVR and SVC\n",
    "bs_svc = bootstrap(n_iterations,n_size,test_embedutts,test_labels,1,\"svm\")\n",
    "bs_svr = bootstrap(n_iterations,n_size,test_embedutts,test_labels,0,\"svm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'SVM Classification 95% Confidence Intervals')\n",
    "print(pd.DataFrame(data=np.transpose(bs_svc),index=schemas,columns=['low','high']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'SVM Regression 95% Confidence Intervals')\n",
    "print(pd.DataFrame(data=np.transpose(bs_svr),index=schemas,columns=['low','high']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We train two types of recurrent neural networks: a multilabel RNN that predicts all 9 schemas simultaneously and a set of 9 single-label RNNs that predict the labels for each schema separately. Each RNN consists of 4 layers: an embedding layer, a bidirectional LSTM layer, a dropout layer, and an output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Multilabel RNN\n",
    "> We used as inspiration for the architecture of all RNNs the paper: Kshirsagar, R., Morris, R., & Bowman, S. (2017). Detecting and explaining crisis. arXiv preprint arXiv:1705.09585. However, we used long short-term memory (LSTM) instead of a gated recurrent unit (GRU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define multilabel model\n",
    "def multilabel_model(train_X, train_y, test_X, test_y,params):\n",
    "    # build the model\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "    #embedding layer\n",
    "    model.add(e)\n",
    "    #LSTM layer\n",
    "    model.add(Bidirectional(LSTM(params['lstm_units'])))\n",
    "    #dropout layer\n",
    "    model.add(Dropout(params['dropout']))\n",
    "    #output layer\n",
    "    model.add(Dense(9, activation='sigmoid'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer=params['optimizer'], loss=params['losses'], metrics=['mean_absolute_error'])\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    # fit the model\n",
    "    out = model.fit(train_X, train_y, \n",
    "                    validation_data=[test_X,test_y],\n",
    "                    batch_size=params['batch_size'], \n",
    "                    epochs=params['epochs'], \n",
    "                    verbose=0)\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(train_X, test_X, train_y, test_y):\n",
    "    #define hyperparameter grid\n",
    "    p={'lstm_units':[50,100],\n",
    "       'optimizer':['rmsprop','Adam'],\n",
    "       'losses':['binary_crossentropy','categorical_crossentropy','mean_absolute_error'],\n",
    "       'dropout':[0.1,0.5],\n",
    "       'batch_size': [32,64],\n",
    "       'epochs':[100]} \n",
    "    #scan the grid\n",
    "    tal=talos.Scan(x=train_X,\n",
    "                   y=train_y,\n",
    "                   x_val=test_X,\n",
    "                   y_val=test_y,\n",
    "                   model=multilabel_model,\n",
    "                   params=p,\n",
    "                   experiment_name='multilabel_rnn',\n",
    "                   print_params=True,\n",
    "                   clear_session=True)\n",
    "    return tal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wall time to run grid search: ~ 2h 10min\n",
    "#run the small grid search\n",
    "%time tal = grid_search(padded_train, padded_validate, train_labels, val_labels)\n",
    "#analyze the outcome\n",
    "analyze_object=talos.Analyze(tal)\n",
    "analysis_results = analyze_object.data\n",
    "#let's have a look at the results of the grid search\n",
    "print(analysis_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we choose the best model of the grid search on the basis of the MAE metric, lower values are better\n",
    "mlm_model = tal.best_model(metric='mean_absolute_error', asc=True)\n",
    "#to get an idea of how our best model performs, we check predictions on the validation set\n",
    "prediction_mlm_val = mlm_model.predict(padded_validate)\n",
    "output_mlm_val = gof_spear(prediction_mlm_val,val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(data=output_mlm_val,index=schemas,columns=['estimate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the predictions make sense considering what we got from KNN and SVM. We deploy the model.\n",
    "talos.Deploy(tal,'mlm_rnn',metric='mean_absolute_error',asc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we restore the deployed Talos experiment\n",
    "restore = talos.Restore('Data/mlm_rnn.zip')\n",
    "#to get the best performing parameters, we get the results of the Talos experiment\n",
    "scan_results = restore.results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select the row with the smallest mean absolute error\n",
    "print(scan_results[scan_results.mean_absolute_error == scan_results.mean_absolute_error.min()]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">We have learned that despite setting the random seed values for numpy and tensorflow, some variability remains with each training of the RNNs and our results will therefore not be 100% reproducible. To ensure that we cannot be accused of reporting just a \"lucky shot\", we have decided to follow the advice given in this blog post https://machinelearningmastery.com/reproducible-results-neural-networks-keras/ . We therefore train 30 multi-label neural nets with the best parameters identfied with the Talos scan. We report the mean Spearman correlations in the article. We do the same with the per-schema RNNs below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlm_fixed(train_X, train_y, test_X, test_y):\n",
    "    # build the model\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "    #embedding layer\n",
    "    model.add(e)\n",
    "    #LSTM layer\n",
    "    model.add(Bidirectional(LSTM(100)))\n",
    "    #dropout layer\n",
    "    model.add(Dropout(0.1))\n",
    "    #output layer\n",
    "    model.add(Dense(9, activation='sigmoid'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['mean_absolute_error'])\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    # fit the model\n",
    "    out = model.fit(train_X, train_y, \n",
    "                    validation_data=[test_X,test_y],\n",
    "                    batch_size=32, \n",
    "                    epochs=100, \n",
    "                    verbose=0)\n",
    "    return out, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 1h 54min\n",
    "for i in range(30):\n",
    "    #we train the model\n",
    "    res, model = mlm_fixed(padded_train, train_labels, padded_validate, val_labels)\n",
    "    #we save models to files to free up working memory\n",
    "    model_name = 'Data/MLMs/mlm_' + str(i)\n",
    "    model.save(model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #generate predictions with the per-schema models\n",
    "def predict_schema_mlm(test_text, test_labels,fixed=None):\n",
    "    if fixed is None:\n",
    "        all_preds = np.zeros((test_labels.shape[0],test_labels.shape[1],30))\n",
    "        all_gofs = np.zeros((30,9))\n",
    "        for j in range(30):\n",
    "            model_name = \"Data/MLMs/mlm_\" + str(j)\n",
    "            model = keras.models.load_model(model_name + '.h5')\n",
    "            preds = model.predict(test_text)\n",
    "            gofs = gof_spear(preds,test_labels)\n",
    "            all_preds[:,:,j] = preds\n",
    "            all_gofs[j,:] = gofs\n",
    "    else:\n",
    "        model_name = \"Data/MLMs/mlm_\" + str(fixed)\n",
    "        model = keras.models.load_model(model_name + '.h5')\n",
    "        all_preds = model.predict(test_text)\n",
    "        all_gofs = gof_spear(all_preds,test_labels)\n",
    "    return all_gofs,all_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Per-Schema RNNs\n",
    "> We also train separate RNNs per schema. For this, we can use the output layer to compute a probability for each of the four possible labels. This way, the labels are treated as separate classes. We take over the parameter values from the multilabel model for the number of LSTM units, the dropout rate, the loss function, the evaluation metric, the batch size, and the number of epochs. To obtain the probability for each class, the units of the output layer have a softmax activation function. For the evaluation, the class with the highest probability is chosen per model. The resulting models are written to files and loaded again for prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define separate models\n",
    "def perschema_models(train_X, train_y, test_X, test_y):\n",
    "    model = Sequential()\n",
    "    e = Embedding(vocab_size, 100, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
    "    model.add(e)\n",
    "    model.add(Bidirectional(LSTM(100)))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    # compile the model\n",
    "    model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['mean_absolute_error'])\n",
    "    # summarize the model\n",
    "    print(model.summary())\n",
    "    # fit the model\n",
    "    model.fit(train_X, train_y,\n",
    "              validation_data=[test_X,test_y],\n",
    "              batch_size=32, \n",
    "              epochs=100, \n",
    "              verbose=0)\n",
    "    out=model.predict(test_X)\n",
    "    gof,p=scipy.stats.spearmanr(out,test_y,axis=None)\n",
    "    return gof, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 16h\n",
    "#train models\n",
    "for j in range(30):\n",
    "    directory_name = \"Data/PSMs/per_schema_models_\" + str(j)\n",
    "    if not os.path.exists(directory_name):\n",
    "        os.makedirs(directory_name)\n",
    "    for i in range(9):\n",
    "        train_label_schema = np_utils.to_categorical(train_labels[:,i])\n",
    "        val_label_schema = np_utils.to_categorical(val_labels[:,i])\n",
    "        val_output_slm, model = perschema_models(padded_train,train_label_schema,padded_validate,val_label_schema)\n",
    "        #we write trained models to files to free up working memory\n",
    "        model_name = '/schema_model_' + schemas[i]\n",
    "        save_model_under = directory_name + model_name\n",
    "        model.save(save_model_under + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load single models\n",
    "def load_single_models(directory):\n",
    "    single_models = []\n",
    "    for i in range(9):\n",
    "        model_name ='/schema_model_' + schemas[i]\n",
    "        get_from = directory + model_name\n",
    "        model = keras.models.load_model(get_from + '.h5')\n",
    "        single_models.append(model)\n",
    "    return single_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate predictions with the per-schema models\n",
    "def predict_schema_psm(test_text, test_labels,fixed=None):\n",
    "    if fixed is None:\n",
    "        all_preds = np.zeros((test_labels.shape[0],test_labels.shape[1],30))\n",
    "        all_gofs = np.zeros((30,9))\n",
    "        for j in range(30):\n",
    "            directory_name = \"Data/PSMs/per_schema_models_\" + str(j)\n",
    "            preds = np.zeros(test_labels.shape)\n",
    "            gofs=[]\n",
    "            single_models = load_single_models(directory_name)\n",
    "            for i in range(9):\n",
    "                model = single_models[i]\n",
    "                out = model.predict(test_text)\n",
    "                out = out.argmax(axis=1)\n",
    "                preds[:,i] = out\n",
    "                gof,p=scipy.stats.spearmanr(out,test_labels[:,i])\n",
    "                gofs.append(gof)\n",
    "            all_preds[:,:,j] = preds\n",
    "            all_gofs[j,:] = gofs\n",
    "    else:\n",
    "        directory_name= \"Data/PSMs/per_schema_models_\" + str(fixed)\n",
    "        all_preds = np.zeros(test_labels.shape)\n",
    "        all_gofs = []\n",
    "        single_models = load_single_models(directory_name)\n",
    "        for i in range(9):\n",
    "            model = single_models[i]\n",
    "            out = model.predict(test_text)\n",
    "            out = out.argmax(axis=1)\n",
    "            all_preds[:,i] = out\n",
    "            gof,p=scipy.stats.spearmanr(out,test_labels[:,i])\n",
    "            all_gofs.append(gof)\n",
    "    return all_gofs,all_preds    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Testset Predictions with the RNN Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_rnn(test_X,test_y,single):\n",
    "    if single:\n",
    "        gof,preds=predict_schema_psm(test_X,test_y)\n",
    "    else:\n",
    "        gof,preds=predict_schema_mlm(test_X,test_y)\n",
    "    #make a sum of all classification values\n",
    "    gof_sum = np.sum(gof,axis=1)\n",
    "    #sort sums\n",
    "    gof_sum_sorted = np.sort(gof_sum)\n",
    "    #pick element that is closest but larger than median (we have even number of elements)\n",
    "    get_med_element = gof_sum_sorted[15]\n",
    "    #get index of median\n",
    "    gof_sum_med_idx = np.where(gof_sum==get_med_element)[0]\n",
    "    #choose this as the final model to use in H2 and to report in the paper\n",
    "    gof_out = gof[gof_sum_med_idx]\n",
    "    return np.transpose(gof_out),gof_sum_med_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 6min\n",
    "# predicting testset with multilabel model\n",
    "output_mlm,idx_mlm = my_rnn(padded_test,test_labels,0)\n",
    "# predicting testset with perschema models\n",
    "output_psm,idx_psm = my_rnn(padded_test,test_labels,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RNN Multilabel Model Testset Output')\n",
    "print(pd.DataFrame(data=output_mlm,index=schemas,columns=['estimate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('RNN Per-Schema Testset Output')\n",
    "print(pd.DataFrame(data=output_psm,index=schemas,columns=['estimate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_rnn_fixed(test_X,test_y,single):\n",
    "    if single:\n",
    "        gof,preds=predict_schema_psm(test_X,test_y,idx_psm[0])\n",
    "    else:\n",
    "        gof,preds=predict_schema_mlm(test_X,test_y,idx_mlm[0])\n",
    "    return gof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# wall time to run: ~ 37min\n",
    "#bootstrapping the 95% confidence intervals\n",
    "bs_mlm = bootstrap(n_iterations,n_size,padded_test,test_labels,0,\"rnn\")\n",
    "bs_psm = bootstrap(n_iterations,n_size,padded_test,test_labels,1,\"rnn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Multilabel RNN Classification 95% Confidence Intervals')\n",
    "print(pd.DataFrame(data=np.transpose(bs_mlm),index=schemas,columns=['low','high']))\n",
    "print(f'Per-Schema RNN Classification 95% Confidence Intervals')\n",
    "print(pd.DataFrame(data=np.transpose(bs_psm),index=schemas,columns=['low','high']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_psm_flat = [item for sublist in output_psm for item in sublist]\n",
    "output_mlm_flat = [item for sublist in output_mlm for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Estimates of all models')\n",
    "outputs = np.concatenate((output_kNN_class,output_kNN_reg,output_SVC, output_SVR, output_psm_flat, output_mlm_flat))\n",
    "outputs=np.reshape(outputs,(9,6),order='F')\n",
    "print(pd.DataFrame(data=outputs,index=schemas,columns=['kNN_class','kNN_reg','SVC','SVR','PSM','MLM']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Lower CIs of all models')\n",
    "lower_cis = np.concatenate((bs_knn_class[0],bs_knn_reg[0],bs_svc[0], bs_svr[0], bs_psm[0], bs_mlm[0]))\n",
    "lower_cis=np.reshape(lower_cis,(9,6),order='F')\n",
    "print(pd.DataFrame(data=lower_cis,index=schemas,columns=['kNN_class','kNN_reg','SVC','SVR','PSM','MLM']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Upper CIs of all models')\n",
    "upper_cis = np.concatenate((bs_knn_class[1],bs_knn_reg[1],bs_svc[1], bs_svr[1], bs_psm[1], bs_mlm[1]))\n",
    "upper_cis=np.reshape(upper_cis,(9,6),order='F')\n",
    "print(pd.DataFrame(data=upper_cis,index=schemas,columns=['kNN_class','kNN_reg','SVC','SVR','PSM','MLM']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Dataset for Testing Hypothesis 2\n",
    "Finally, we need to use the best-performing algorithm, the per-schema RNNs, to generate the predictions on the testset and write these to a file so that we can use them to test Hypothesis 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gofH2,predsH2=predict_schema_psm(padded_test,test_labels,idx_psm[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsH2 = predsH2.astype(int)\n",
    "print(predsH2[:,0:5])\n",
    "diag_rho = [scipy.stats.spearmanr(predsH2[i,:], test_labels[i,0:9], nan_policy='omit')[0] for i in range(predsH2.shape[0])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predsH2 = pd.DataFrame(data=predsH2,columns=['AttachPred','CompPred',\"GlobalPred\",\"HealthPred\",\"ControlPred\",\"MetaCogPred\",\"OthersPred\",\"HopelessPred\",\"OthViewsPred\"])\n",
    "df_predsH2[\"Corr\"] = pd.DataFrame(diag_rho)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_predsH2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predsH2.to_csv(\"Data/PredictionsH2.csv\", sep=';', header=True, index=False, mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3env",
   "language": "python",
   "name": "py3env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
